# -*- coding: utf-8 -*-
"""Maria_Humberto_Giao_ISY503 A3_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SDi8zLJfiWQ7lKvE-lg9E2zkYLGpdQ8u

# **ISY503 Assessment 3**
NLP Group Project: Maria Clare - Humberto Rodrigues- Zoe Reynolds
"""

# Importing libraries
import pandas as pd
import numpy as np
import re
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import style
style.use('ggplot')
from textblob import TextBlob
from wordcloud import WordCloud
import nltk
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
!pip install scikit-learn
import sklearn
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""### **1. Data Collection**
*   Importing Negative Reviews
*   Importing Positive reviews

Importing All Negative Reviews
"""

# Importing BOOKS NEGATIVE
from google.colab import files
upload = files.upload()

book_neg = open ('books.negative.review', 'r').read()
display(book_neg)

# Importing DVD NEGATIVE
from google.colab import files
upload = files.upload()

dvd_neg = open ('dvd.negative.review', 'r').read()
display(dvd_neg)

# Importing ELETRONICS NEGATIVE
from google.colab import files
upload = files.upload()

elec_neg = open ('electronics.negative.review', 'r').read()
display(elec_neg)

# Importing KITCHEN NEGATIVE
from google.colab import files
upload = files.upload()

kit_neg = open ('kitchen.negative.review', 'r').read()
display(kit_neg)

# Putting all negative reviews together

neg_reviews=" ".join((book_neg, dvd_neg, elec_neg,kit_neg))

type(neg_reviews)
display(neg_reviews)

"""Importing All Positive Reviews

"""

# Importing BOOK POSITIVE
from google.colab import files
upload = files.upload()

book_pos = open('books.positive.review','r'). read()
display(book_pos)

# Importing DVD POSITIVE
from google.colab import files
upload = files.upload()

dvd_pos = open('dvd.positive.review','r'). read()
display(dvd_pos)

# Importing ELETRONICS POSITIVE
from google.colab import files
upload = files.upload()

elec_pos = open('electronics.positive.review','r'). read()
display(elec_pos)

# Importing KITCHEN POSITIVE
from google.colab import files
upload = files.upload()

kit_pos = open('kitchen.positive.review','r'). read()
display(kit_pos)

#Joining all POSITIVES together

pos_reviews=" ".join((book_pos, dvd_pos, elec_pos, kit_pos))
type(pos_reviews)
display(pos_reviews)

"""### **2. Preparing Data for Modelling**
*   Cleansing Data
*   Pre-processing Data
*   Tokenizing




"""

# setting all strings to lower case
neg_reviews=neg_reviews.lower()
pos_reviews=pos_reviews.lower()

display(neg_reviews)
display(pos_reviews)

# proceed with negative words
# importing libraries
import re
from nltk.tokenize import RegexpTokenizer

# removing underscores and slashes
neg_reviews1=neg_reviews.replace("_"," ").replace("/","")

# removing number
neg_reviews1= re.sub(r"\b[0-9]+\b\s*","", neg_reviews1)

# remove hyperlinks
neg_reviews1 = re.sub(r"http?://\S+", "", neg_reviews1)

# remove the <a> tags
neg_reviews1 = re.sub(r"<a[^>]*>(.*?)</a>", r"\1", neg_reviews1)

# remove the html tags but keeep their contents
neg_reviews1 = re.sub(r"<.*?>", " ", neg_reviews1)

# remove the alphanumerics
neg_reviews1 = re.sub(r'\w\d\w*', '', neg_reviews1)

# remove undesire punctuation
neg_tokens = RegexpTokenizer(r'\w+' ).tokenize(neg_reviews1)

print(neg_tokens)
type(neg_tokens)

# proceeding with positive words

# removing underscores and slashes
pos_reviews1=pos_reviews.replace("_"," ").replace("/","")

# removing number
pos_reviews1= re.sub(r"\b[0-9]+\b\s*","", pos_reviews1)

# removing hyperlinks
pos_reviews1 = re.sub(r"http?://\S+", "", pos_reviews1)

# removing the <a> tags
pos_reviews1 = re.sub(r"<a[^>]*>(.*?)</a>", r"\1", pos_reviews1)

# removing the html tags but keeep their contents
pos_reviews1 = re.sub(r"<.*?>", " ", pos_reviews1)

# removing the aplhanumerics
pos_reviews1 = re.sub(r'\w\d\w*', '', pos_reviews1)

# removing undesire punctuation
pos_tokens = RegexpTokenizer(r'\w+' ).tokenize(pos_reviews1)

print(pos_tokens)
type(pos_tokens)

"""### **3. Removing Stopwords**"""

# importing libraries
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords

# removing stopwords for negative tokens

english_stopwords = stopwords.words('english')
neg_tokens_wo_stopwords = [t for t in neg_tokens if t not in english_stopwords]
print ("Negative Reviews with stopwords: ", " ".join(neg_tokens_wo_stopwords))
type(neg_tokens_wo_stopwords)

# revoming additional stopwords

stop_list = ["n't","'s",".","-","ee","etc","az","e","p","ca","sf","l","ab","onoff","er","f","r","rj","k","ny","m","i","g","s","d",]
stpwrd=nltk.corpus.stopwords.words('english')
stpwrd.extend(stop_list)
print(stpwrd)

# removing stopwords with the extended list of stopwords for negative tokens

neg_tk_wo_stopwords = [n for n in neg_tokens if n not in stpwrd]


print("Negative reviews without stopwords:", " ".join(neg_tk_wo_stopwords))


print(type(neg_tk_wo_stopwords))

# removing stopwords for positive tokens

english_stopwords = stopwords.words('english')
pos_tokens_wo_stopwords = [t for t in pos_tokens if t not in english_stopwords]
print ("Positive Reviews with stopwords: ", " ".join(pos_tokens_wo_stopwords))
type(pos_tokens_wo_stopwords)

#revoming additional stopwords

stop_list = ["n't","'s",".","-","ee","etc","az","e","p","ca","sf","l","ab","onoff","er","f","r","rj","k","ny","m","i","g","s","d",]
stpwrd=nltk.corpus.stopwords.words('english')
stpwrd.extend(stop_list)
print(stpwrd)

#remove stopwords with the extended list of stopwords for positive tokens

pos_tk_wo_stopwords = [p for p in pos_tokens if p not in stpwrd]


print("Positive reviews without stopwords:", " ".join(pos_tk_wo_stopwords))


print(type(pos_tk_wo_stopwords))

# removing stopwrods with the extended list of stopwords for positive tokens

pos_tk_wo_stpwrds = [p for p in pos_tokens if p not in stpwrd]
print("Positive reviewes without stopwords ", " ".join(pos_tk_wo_stpwrds))
type(pos_tk_wo_stpwrds)

"""### **4. Chunking**"""

# importing libraries
nltk.download('averaged_perceptron_tagger')

# defining grammar using regular expressions

grammar = ('''
    NP: {<DT>?<JJ>*<NN>}) # NP
    ''')

"""### **5. POS Tagging**"""

# tagging negative tokens
tag_neg = nltk.pos_tag(neg_tokens_wo_stopwords)
# tagging positive tokens
tag_pos = nltk.pos_tag(pos_tk_wo_stpwrds)

tag_pos,tag_neg

# importing libraries
from collections import Counter

# counting and printing counts of negative / positive words
counts_neg = Counter(tag for word, tag in tag_neg)
counts_pos = Counter(tag2 for word, tag2 in tag_pos)

print("Count_neg:", counts_neg)
print("Count_pos:", counts_pos)

"""### **6. Lemmatization**"""

# importing libraries
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer

# applying Lemmatizer function
lemmatizer = WordNetLemmatizer()

lemmatized_neg = ' '.join([lemmatizer.lemmatize(h) for h in neg_tk_wo_stopwords])
print(lemmatized_neg)
type(lemmatized_neg)

lemmatized_pos = ' '.join([lemmatizer.lemmatize(h) for h in pos_tk_wo_stpwrds])
print(lemmatized_pos)
type(lemmatized_pos)

"""### **7. Frequency Distribution**"""

# importing libraries
from nltk.probability import FreqDist

# converting data string into list
neg_words = lemmatized_neg.split()
pos_words = lemmatized_pos.split()
print(neg_words)
print(pos_words)

# checking the frequency distribution of both datasets
fdist_neg = nltk.FreqDist(neg_words)
print(fdist_neg)

fdist_pos= nltk.FreqDist (pos_words)
print(fdist_pos)

# checking most repeated words
fdist_neg.most_common(8)
fdist_neg.tabulate(8)

fdist_pos.most_common(8)
fdist_pos.tabulate(8)

# filtering only truly negative and truly positive

common_set = set(fdist_pos).intersection(fdist_neg)
for word in common_set:
    del fdist_pos[word]
    del fdist_neg[word]
top_100_positive = {word for word, count in fdist_pos.most_common(100)}
top_100_negative = {word for word, count in fdist_neg.most_common(100)}

# showing top 100 positive / negative words
print(top_100_positive)

print(top_100_negative)

# printing frequency distribution
print(fdist_pos)
print(fdist_neg)

# exploring most commmon negative words
fdist_neg.most_common(8)
fdist_neg.tabulate(8)
print(fdist_neg)

# exploring most commmon positive words
fdist_pos.most_common(8)
fdist_pos.tabulate(8)
print(fdist_pos)

"""### **8. Converting Tokens to a Dictionary**"""

# for positive dataset
positive_dataset = [(rev_dict, "Positive") for rev_dict in pos_words]
print(positive_dataset)

# for negative dataset
negative_dataset = [(rev_dict, "Negative") for rev_dict in neg_words]
print(negative_dataset)

"""### **9. Creating and Evaluating Model**
*   Vectorization
*   Model Compilation
*   Data Splitting and Preprocessing
*   Model Training
*   Model Testing
*   Model Evaluation

Joining both Negative and Positive into One Dataset
"""

# importing libraries
import random

# combining in one dataset
dataset_reviews = positive_dataset + negative_dataset
random.shuffle(dataset_reviews)

# showing dataset
print(dataset_reviews)

# exploring dataset
type(dataset_reviews)

# creating data frame
import pandas as pd
df_reviews=pd.DataFrame(dataset_reviews)
df_reviews.columns = ['Word' , 'Sentiment']
print(df_reviews)

# visualising data frame
fig = plt.figure(figsize=(5,5))
sns.countplot(x='Sentiment', data = df_reviews)
df_reviews.groupby('Sentiment').size().plot(kind='bar')
plt.show()

# visualising data frame
fig = plt.figure(figsize=(7,7))
colors = ("yellowgreen", "gold", "red")
wp = {'linewidth':2, 'edgecolor':"black"}
tags = df_reviews['Sentiment'].value_counts()
explode = (0.1, 0.1)
tags.plot(kind='pie', autopct='%1.1f%%', shadow=True, colors=colors,
          startangle=90, wedgeprops=wp, explode=explode, label='')
plt.title('Distribution of Sentiment')
plt.show()

X = df_reviews.iloc[:,0].values
y = df_reviews.iloc[:,1].values

# applying vectorization
vect = CountVectorizer(ngram_range=(1,1)).fit(df_reviews['Word'])

# defining variables
X = df_reviews['Word']
y = df_reviews['Sentiment']
X = vect.transform(X)

# importing libraries to split dataset for data training and testing
import numpy as np
from sklearn.model_selection import train_test_split

# splitting dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# exploring train / test datasets
print("Size of X_train:", (X_train.shape))
print("Size of y_train:", (y_train.shape))
print("Size of X_test:", (X_test.shape))
print("Size of y_test:", (y_test.shape))

"""##### Using LinearSCV Model"""

# importing libraries for LinearSCV
from sklearn.svm import LinearSVC

# fitting LinearSCV model to train dataset
SVCmodel = LinearSVC()
SVCmodel.fit(X_train, y_train)

# applying LinearSCV to test dataset and creating Accuracy score
svc_pred = SVCmodel.predict(X_test)
svc_acc = accuracy_score(svc_pred, y_test)
print("Test Accuracy:{:2f}%".format(svc_acc*100))

# printing Confusion matrix and Classification report
print(confusion_matrix(y_test, svc_pred))
print("\n")
print(classification_report(y_test, svc_pred))

"""##### Using Logistic Regression Model"""

# supressing certain warning messages
import warnings
warnings.filterwarnings('ignore')

# fitting the Logistic Regression model to train dataset
# creating Accuracy score
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

# Predicting on the test data
logreg_pred = logreg.predict(X_test)

# Evaluating the model accuracy
logreg_acc = accuracy_score(y_test, logreg_pred)
print("Test Accuracy:{:2f}%".format(logreg_acc*100))

# using alternative model evaluation
style.use('classic')
cm = confusion_matrix(y_test, logreg_pred, labels=logreg.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)
disp.plot()
plt.show()

# applying hyperparameter tuning
# importing library
from sklearn.model_selection import GridSearchCV
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}
grid = GridSearchCV(LogisticRegression(), param_grid)
grid.fit(X_train, y_train)

# exploring model best parameters
print("Best Parameters:", grid.best_params_)
print("Best Score:", grid.best_score_)

# evaluating model - Accurace score
y_pred = grid.predict(X_test)
logreg_acc = accuracy_score(y_test, y_pred)
print("Test Accuracy:{:2f}%".format(logreg_acc*100))

# evaluating model - Confusion matrix and Classification report
print(confusion_matrix(y_test, y_pred))
print("\n")
print(classification_report(y_test, y_pred))

# applying hyperparameter tuning using grid

grid = {'C': [0.01, 0.1, 1, 10],
        'kernel': ["linear", "rbf", "poly", "sigmoid"],
        'gamma': [0.01, 1],
        'degree': [1, 3, 5, 7]}
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}
grid = GridSearchCV(LogisticRegression(), param_grid)
grid.fit(X_train, y_train)

# exploring model best parameters
print("Best Parameters:", grid.best_params_)
print("Best Score:", grid.best_score_)

# evaluating model - Accurace score
y_pred = grid.predict(X_test)
logreg_acc = accuracy_score(y_test, y_pred)
print("Test Accuracy:{:2f}%".format(logreg_acc*100))

# evaluating model - Confusion matrix and Classification report
print(confusion_matrix(y_test, y_pred))
print("\n")
print(classification_report(y_test, y_pred))

"""### **10. Creating Executable**

#### Using pyinstaller
"""

# importing library
!pip install pyinstaller

print("maria_humberto_giao_isy503_a3_code.py")

!pyinstaller --onefile maria_humberto_giao_isy503_a3_code.py

"""#### Using auto-py-to-exe"""

# importing libraries
!pip install auto-py-to-exe

# upgrading model
!pip install --upgrade auto_py_to_exe

# showing library
!pip show auto_py_to_exe

# converting .py to .exe file
!convert .py to .exe
!auto_py_to_exe.convert_file("maria_humberto_giao_isy503_a3_code.py")

# printing .exe file
print("maria_humberto_giao_isy503_a3_code.exe")
